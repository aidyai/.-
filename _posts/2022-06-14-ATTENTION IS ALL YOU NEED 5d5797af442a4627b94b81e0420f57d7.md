# ATTENTION IS ALL YOU NEED

**`TRANFORMERS EXPLAINED`**

                                             **`tranformers.png`**

- **INPUTS:**
    
    **input fed into the TRANSFOMER:** The first step is basically input data processing, since computers donâ€™t understand Images, Texts as we do we have to transform them into what it understands (Matrices or Numbers). E.g Given a piece of text: **I will achieve more that what Elon Musk and Issac Newton have achieved through,** we want to generate a proper corresponding completion for this.
    
    **What w**e do is we take the example sentence above and create a vocabulary dictionary out of it. **Then** we assign a numeric index next to each word. **We then** pick the numeric index that corresponds to the word in our vocab dictionary for all the words found in our example sentence. **Consequ**ently what gets fed into a Transformer is not the words but their corresponding indices. **[22,333,45,6677,889,2,22,2244,45,566,667,88,334,3,3].**
    
    This **[22,333,45,6677,889,2,22,2244,45,566,667,88,334,3,3]** will be passed on to the next Layer which is the Embedding Layer
    
- **EMBEDDING LAYER:**
    
    Embedding layer takes input indices and converts them into WORD EMBEDDINGs which is passed on to the next layer. 
    
    The embedding Layer too has an Index for every word in the Vocab Dictionary and against each of those indices a Vector is attached like so:
    
    [https://www.notion.so](https://www.notion.so)
    
    Initially, the **vector** attached above are filled with completely random numbers but during **Model Training** it will be updated **.** Additionally, the word embedding size used in the Original paper is **512.**
    
    **WORD EMBEDDINGs:** Its the Vector representation of a given word. Each dimension of the word embedding tries to capture some linguistic feature about the word. The model decides this features itself during training from a randomly initialized features. The values of each dimension represent the coordinates of the given word in an hyperspace. 
    
     
    
    [https://www.notion.so](https://www.notion.so)
    
    If two words share similar linguistic features, and they appear in similar context, their embedding values will tend to become closer and closer during the training process
    
    Example: The word **games & play** at first their embeddings are randomly initialized but during the course of training the two words will become more similar because the both words usually appear in similar contexts (scenario) as compare to a word **Storey-Building** which will always appear in a very different context.
    
    [https://www.notion.so](https://www.notion.so)
    
- **POSITIONAL EMBEDDING**
    
    Firstly, positions of word matters because any shift in word order could change the sentiment of a sentence, consequently the meaning and as such Sequential models like LSTMs picks the word embeddings sequentially which in turn make it better able to retain the sentiment and order in sentences and unfortunately making training slow. Transformers on the other hand, takes the whole word embeddings at once and making training first at the expense of losing the order of word in a sentences not knowing which word came first e.t.c. The concept of Positional Embedding was introduced to tackle this problem of bringing word ordering back to Transformers without making it Recurrent like LSTMs and it works like this:
    
    1. Use Wave Frequencies **(sin curves*& cosine curves*)**to capture Positional Information, the sin formula was used for the odd positions to get the position embeddings while the cosine likewise
        
        $$
        PE_(pos,2i_)= sin(pos/1000^2i_/d )                             
        $$
        
        ---
        
        $$
        \\ PE_(pos, _2i+1_)= cos(pos/1000 ^2i /d )
        $$
        
        ---
        
          `where: p= position`     `d = word embedding size;`     `i = indices of each of the embedding position dimension`
        
    
    ---
    
    [https://www.notion.so](https://www.notion.so)
    
    1. Then add the Positional embeddings to the word embeddings like so; to get a Positional aware Word Embeddings
    
    [https://www.notion.so](https://www.notion.so)
    
    - **SUMMARY;**
        1. The Inputs are converted to **WORD EMBEDDINGS (WE).**
        2. Then a **POSITIONAL EMBEDDINGS** is introduced where both **(WE & PE)** are added together resulting in a Positional aware Word Embeddings.
        3. 
        
        [https://www.notion.so](https://www.notion.so)
        
- **MULTI-HEAD ATTENTION**
    
                      Firstly; **Why do we need Attention**? The Attention Mechanism helps the Model to focus on important words in a given input sentence